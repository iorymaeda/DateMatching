{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdc3ca6-d1bb-4de5-840d-17b31eb7a6ac",
   "metadata": {},
   "source": [
    "There i parse telegram chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de57cd9-4023-495e-87ef-4b57c28922ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.9.0-cp310-cp310-win_amd64.whl (3.6 MB)\n",
      "     ---------------------------------------- 3.6/3.6 MB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-4.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\royta\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "22e3606b-45c5-4e8e-baaa-7a813cebc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "from lxml import html\n",
    "from contextlib import suppress\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f5e8f51-ad44-4d7f-a48d-9d261efd4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(body: html.HtmlElement) -> str|None:\n",
    "    with suppress(Exception):\n",
    "        name = body.find_class('from_name')[0].text\n",
    "        name = name.replace('\\n', '').strip()\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0bd8d39-013c-4a9f-85be-41e033a44702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo(body: html.HtmlElement) -> str|None:\n",
    "    with suppress(Exception):\n",
    "        div = body.find_class('media_wrap')[0]\n",
    "        a = div.getchildren()[0]\n",
    "        photo = a.attrib['href']\n",
    "        return photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9541058-6739-4fa0-af88-f0db0445140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(body: html.HtmlElement, text=\"\") -> str|None:\n",
    "    with suppress(Exception):\n",
    "        div = body.find_class('text')[0]\n",
    "        for t in div.itertext():\n",
    "            text+= t\n",
    "            \n",
    "        text = text.replace('\\n', '', 2).strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d34c5777-42f1-4ef2-b211-a9a2648cf2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "for catalog in os.listdir('tg_data'):\n",
    "    \n",
    "    for file in os.listdir('tg_data/' + catalog):\n",
    "        if '.html' in file:\n",
    "\n",
    "            with open(f\"tg_data/{catalog}/{file}\", \"r\", encoding='utf-8') as f:\n",
    "                _html = f.read()\n",
    "\n",
    "            tree = html.fromstring(_html)\n",
    "            history = tree.xpath(\"/html/body/div/div[2]/div\")[0]\n",
    "            history = history.find_class('message default clearfix')\n",
    "\n",
    "            for message in history:\n",
    "                body = message.find_class('body')[0]\n",
    "                body = message.find_class('body')[0]\n",
    "                    \n",
    "                photo = get_photo(body) \n",
    "                photo = f\"{catalog}/{photo}\" if photo else None\n",
    "                messages+= [{\n",
    "                    'name': get_name(body),\n",
    "                    'text': get_text(body),\n",
    "                    'photo': photo,\n",
    "                }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71f24ab5-a49e-4cb5-adb0-f5eb1e62f25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111M/111M [02:53<00:00, 667kB/s]\n"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN()\n",
    "model1 = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "model2 = InceptionResnetV1(pretrained='casia-webface').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "535f4d6c-8d46-4f49-a9af-a8ca6500d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_models = [model1, model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ab6318f2-126d-4416-8707-cca5ce49f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoEmbedingStorage:\n",
    "    \"\"\"This storage class contains \n",
    "    primary photo path+name: generated unique id \n",
    "    generated unique id: [model1 embeddings, ..., modelN embeddings]\n",
    "    \"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        self.path_id: dict[str, int] = {}\n",
    "        self.id_path: dict[int, str] = {}\n",
    "        self.id_embs: dict[int, np.ndarray] = {}\n",
    "        \n",
    "        if path:\n",
    "            self.load()\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        \"Save storage to path\"\n",
    "        if path is None:\n",
    "            if self.path:\n",
    "                path = self.path\n",
    "            else:\n",
    "                raise Exception('Provide path to save')\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'path_id': self.path_id,\n",
    "                'id_path': self.id_path,\n",
    "                'id_embs': self.id_embs,\n",
    "            }, f)\n",
    "            \n",
    "                \n",
    "    def load(self, path=None):\n",
    "        \"Load storage to path\"\n",
    "        if path is None:\n",
    "            if self.path:\n",
    "                path = self.path\n",
    "            else:\n",
    "                raise Exception('Provide path to save')\n",
    "    \n",
    "        with open(path, 'rb') as f:\n",
    "            loaded_dict = pickle.load(f)\n",
    "            self.path_id = loaded_dict['path_id']\n",
    "            self.id_path = loaded_dict['id_path']\n",
    "            self.id_embs = loaded_dict['id_embs']\n",
    "            \n",
    "            \n",
    "    def __generate_id(self) -> int:\n",
    "        return len(self.path_id)\n",
    "    \n",
    "    \n",
    "    def __setitem__(self, key: str, value: list[np.ndarray|torch.Tensor]):\n",
    "        \"\"\"Put embeddings in storage\n",
    "        :param key: primary photo path+name\n",
    "        :param value: list with embeddings from models\n",
    "        \"\"\"\n",
    "        assert isinstance(key, str)\n",
    "        assert isinstance(value, list)\n",
    "        \n",
    "        for emb in value:\n",
    "            assert isinstance(emb, (np.ndarray, torch.Tensor))\n",
    "        \n",
    "        uid = self.__generate_id()\n",
    "        self.path_id[key] = uid\n",
    "        self.id_path[uid] = key\n",
    "        self.id_embs[uid] = embs\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, item: tuple[int|str, int]) -> np.ndarray:\n",
    "        \"\"\"Get embeddings from storage\n",
    "        if item is tuple, there are key and n\n",
    "        :param key: id or primary path\n",
    "        :param n: since we have different embeddings\n",
    "            from different models, this argument responsible \n",
    "            for number of returned embedding, N - embedding from N+1 model.\n",
    "            default: 0\n",
    "        \"\"\"\n",
    "        if isinstance(item, tuple):\n",
    "            key, n = item\n",
    "        else:\n",
    "            key, n = item, 0\n",
    "        \n",
    "        if isinstance(key, int):\n",
    "            return self.id_embs[key][n]\n",
    "        \n",
    "        elif isinstance(key, str):\n",
    "            uid = self.path_id[key]\n",
    "            return self.id_embs[uid][n]\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(f'Dont understand key type: {type(key)}')\n",
    "            \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.path_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a0e0ff2b-4ead-49c5-80c3-fbc3398539ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeder = PhotoEmbedingStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c71919f7-7f7a-46fd-9d74-39cff3dbc6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_photo(photo_path:str, saved_path:str):\n",
    "    img = Image.open(photo_path)\n",
    "    img_cropped = mtcnn(img, save_path='tmp.png').unsqueeze(0)\n",
    "\n",
    "    embs = [model(img_cropped) for model in embeds_models]\n",
    "    embeder[photo_path] = embs\n",
    "\n",
    "    uid = embeder.path_id[photo_path]\n",
    "    shutil.move('tmp.png', f'{saved_path}/{uid}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9af8202d-22fc-404c-a99e-49da7319c054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for current, previous in zip(messages[1:], messages[:-1]):\n",
    "        if previous['photo']:\n",
    "            photo_path = 'tg_data/' + previous['photo']\n",
    "\n",
    "            if current['text'] in ['üíå', '‚ù§Ô∏è', 'üëç']:\n",
    "                saved_path = 'data/target'\n",
    "\n",
    "            elif current['text'] in ['üëé']:\n",
    "                saved_path = 'data/opposite'\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "            \n",
    "            process_photo(photo_path, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4b1be-9304-4b31-84c2-5eda88bde1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeder.save('emb storage.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dada54ed-9542-4fb1-a713-7b79a979a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8839ed-6492-40ef-9e1c-e026e37e92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procces_markup(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9ef295-5e72-4ded-badd-b153717b5c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\royta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('markup/test_target'):\n",
    "    photo_path = 'markup/test_target/' + file\n",
    "    photo_name = file\n",
    "    saved_path = DATASET_PATH + \"/test_target/\" + file\n",
    "    \n",
    "    img = Image.open(photo_path)\n",
    "    _, e = file.split('.')\n",
    "    if e.lower() == 'png':\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "    img_cropped = mtcnn(img, save_path=saved_path)\n",
    "    \n",
    "    \n",
    "for file in os.listdir('markup/test_opposite'):\n",
    "    photo_path = 'markup/test_opposite/' + file\n",
    "    photo_name = file\n",
    "    saved_path = DATASET_PATH + \"/test_opposite/\" + file\n",
    "\n",
    "    img = Image.open(photo_path)\n",
    "    _, e = file.split('.')\n",
    "    if e.lower() == 'png':\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "    img_cropped = mtcnn(img, save_path=saved_path)\n",
    "    \n",
    "    \n",
    "for file in os.listdir('markup/target'):\n",
    "    photo_path = 'markup/target/' + file\n",
    "    photo_name = file\n",
    "    saved_path = DATASET_PATH + \"/markup_target/\" + file\n",
    "    \n",
    "    img = Image.open(photo_path)\n",
    "    _, e = file.split('.')\n",
    "    if e.lower() == 'png':\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "    img_cropped = mtcnn(img, save_path=saved_path)\n",
    "    \n",
    "    \n",
    "for file in os.listdir('markup/opposite'):\n",
    "    photo_path = 'markup/opposite/' + file\n",
    "    photo_name = file\n",
    "    saved_path = DATASET_PATH + \"/markup_opposite/\" + file\n",
    "    \n",
    "    img = Image.open(photo_path)\n",
    "    _, e = file.split('.')\n",
    "    if e.lower() == 'png':\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "    img_cropped = mtcnn(img, save_path=saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9fc83-f9fb-4738-ac9b-4433f2f2c1cc",
   "metadata": {},
   "source": [
    "Manually clear the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
